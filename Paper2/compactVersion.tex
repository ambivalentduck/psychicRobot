\documentclass[10pt]{article}

% amsmath package, useful for mathematical formulas
\usepackage{amsmath}
% amssymb package, useful for mathematical symbols
\usepackage{amssymb}

% graphicx package, useful for including eps and pdf graphics
% include graphics with the command \includegraphics
\usepackage{graphicx}
\usepackage{color} 

% Text layout
\topmargin 0.0cm
\oddsidemargin 0.5cm
\evensidemargin 0.5cm
\textwidth 16cm 
\textheight 21cm

% Bold the 'Figure #' in the caption and separate it with a period
% Captions will be left justified
\usepackage[labelfont=bf,labelsep=period,justification=raggedright]{caption}

\pagestyle{myheadings}

\begin{document}
\section*{Why Endure the Math?}
By making two assumptions, we're able to derive human learning and planning from first principles. Moreover, we develop the tools needed to verifying our predictions incidentally along the way.

\section*{Linear Independence of Intent and Process Uncertainty}
When performing intent extraction, we're faced with many significant unknowns. Certain arm parameters can be changed volitionally (humans can co-contract their muscles, raising the arm's stiffness) while others might diverge from cadaver anatomy as studied in the 1950s. We derive the relationship between extractions with different parameters in the hope of finding components of the extraction that relate primarily to estimation error in some parameter value. With such a ``signature'' in hand, we can subtract a scaled version of this signature off our estimated intent to recover a better estimate of the true intent.

We begin by noting that the arm has four sets of parameters at any point in time: two in our model ($M_e, B_e, K_e$ and $M_d, B_d, K_d$), one in reality ($M, B, K$), and one estimated by the brain ($\hat{M}, \hat{B}, \hat{K}$). These are related through a pair of force balances where $f$ is measured force, $x$ is measured arm position, $y$ is intent, and $y_e$ is estimated intent:
\begin{equation}
M\ddot{x}+B\dot{x}+Kx+f=\hat{M}\ddot{y}+\hat{B}\dot{y}+\hat{K}y
\end{equation}
\begin{equation}
M_e\ddot{x}+B_e\dot{x}+K_ex+f=M_d\ddot{y}_e+B_d\dot{y}_e+K_dy_e
\end{equation}
We move to the LaPlace domain for ease of representation and solve each equation for F.
\begin{equation}
F=(\hat{M}s^2+\hat{B}s+\hat{K})Y-(Ms^2+Bs+K)X
\end{equation}
\begin{equation}
F=(M_ds^2+B_ds+K_d)Y_e-(M_es^2+B_es+K_e)X
\end{equation}
Set these equal:
\begin{equation}
(\hat{M}s^2+\hat{B}s+\hat{K})Y-(Ms^2+Bs+K)X=(M_ds^2+B_ds+K_d)Y_e-(M_es^2+B_es+K_e)X
\end{equation}
Since we desire to find the difference of $y$ and $y_e$, we collect terms and define two terms whose sole purpose is to make our notation more convenient for addressing the question at hand. First we define the difference in estimated arm models:
\begin{equation}
\epsilon_i=(\hat{M}s^2+\hat{B}s+\hat{K})-(M_ds^2+B_ds+K_d)
\end{equation}
Next we define the difference in arm models:
\begin{equation}
\epsilon_f=(Ms^2+Bs+K)-(M_es^2+B_es+K_e)
\end{equation}
Collect terms:
\begin{equation}
(\hat{M}s^2+\hat{B}s+\hat{K})(Y-Y_e)+\epsilon_i Y_e=\epsilon_f X
\end{equation}
And rearrange:
\begin{equation}
Y_e=Y+\frac{\epsilon_i Y_e-\epsilon_f X}{\hat{M}s^2+\hat{B}s+\hat{K}}
\end{equation}
While on first glance this appears to contain a tremendous number of unknowns, $y_e$ is locally-in-time linear in each unknown. Therefore, the problem simplifies to a linear regression where the term that varies with no parameters represents the true intent, $y(t)$. We can therefore learn the true intent and parameter values simultaneously by extracting the intent many times with a carefully chosen variety of parameters. Moreover, it appears that this should be true of any system designed to drive a process along a reference trajectory.

This suggests a process with two stages:
\begin{enumerate}
\item Create many candidate estimations of the intent using the Sobol set to carefully span a reasonable parameter space.
\item Use adaptive filtering to recover intent and parameter values as they vary in time.
\end{enumerate}

\section*{Intent as a Sum of Kernels}
Based on the results of Flash and Hogan and Flash and Henis, one might reasonably set out to model intent as the sum of $5^{th}$-order polynomial submovements. We make our first assumption:
\begin{quote}
\textbf{Intent is well-modeled as vector-summed subunits that evolve in time as minimum-jerk, $5^{th}$ order polynomials.}
\end{quote}
This has some immediate implications.



\section*{T}
Stuff

\end{document}
